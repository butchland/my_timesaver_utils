{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp bayes_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Inference \n",
    "\n",
    "> * adding a measure of uncertainty to predictions\n",
    "\n",
    "\n",
    "* useful for detecting out of distribution (OOD) samples in your data\n",
    "* works without an OOD sample dataset\n",
    "* works with existing trained models\n",
    "* tradeoff: slower inference due to sampling over distribution\n",
    "* behind the scenes -> uses the MonteCarlo Dropout Callback (MCDropoutCallback) \n",
    "* based on the article : [Bayesian deep learning with Fastai : how not to be uncertain about your uncertainty !](https://towardsdatascience.com/bayesian-deep-learning-with-fastai-how-not-to-be-uncertain-about-your-uncertainty-6a99d1aa686e)\n",
    "* and on the [github code](https://github.com/dhuynh95/fastai_bayesian) by Daniel Huynh\n",
    "* updated for fastai v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ci\n",
    "#hide\n",
    "!pip install -Uqq fastai --upgrade\n",
    "!pip install -Uqq seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local\n",
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.preds import MCDropoutCallback\n",
    "from fastai.learner import Learner\n",
    "from fastcore.foundation import patch, L\n",
    "from fastai.torch_core import to_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Metrics\n",
    "\n",
    "This is modified  from fastai_bayesian github code by Daniel Huynh \n",
    "but modified to use Pytorch tensors instead of Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def entropy(probs):\n",
    "    \"\"\"Return the prediction of a T*N*C tensor with :\n",
    "        - T : the number of samples\n",
    "        - N : the batch size\n",
    "        - C : the number of classes\n",
    "    \"\"\"\n",
    "    mean_probs = probs.mean(dim=0)\n",
    "    entrop = - (torch.log(mean_probs) * mean_probs).sum(dim=1)\n",
    "    return entrop\n",
    "\n",
    "def uncertainty_best_probability(probs):\n",
    "    \"\"\"Return the standard deviation of the most probable class\"\"\"\n",
    "    idx = probs.mean(dim=0).argmax(dim=1)\n",
    "\n",
    "    std = probs[:, torch.arange(len(idx)), idx].std(dim=0)\n",
    "\n",
    "    return std\n",
    "\n",
    "def BALD(probs):\n",
    "    \"\"\"Information Gain, distance between the entropy of averages and average of entropy\"\"\"\n",
    "    entrop1 = entropy(probs)\n",
    "    entrop2 = - (torch.log(probs) * probs).sum(dim=2)\n",
    "    entrop2 = entrop2.mean(dim=0)\n",
    "\n",
    "    ig = entrop1 - entrop2\n",
    "    return ig\n",
    "\n",
    "def top_k_uncertainty(s, k=5, reverse=True):\n",
    "    \"\"\"Return the top k indexes\"\"\"\n",
    "    sorted_s = sorted(list(zip(torch.arange(len(s)), s)),\n",
    "                      key=lambda x: x[1], reverse=reverse)\n",
    "    output = [sorted_s[i][0] for i in range(k)]\n",
    "    \n",
    "def plot_hist_groups(pred,y,metric,bins=None,figsize=(16,16)):\n",
    "    TP = to_np((pred.mean(dim=0).argmax(dim=1) == y) & (y == 1))\n",
    "    TN = to_np((pred.mean(dim=0).argmax(dim=1) == y) & (y == 0))\n",
    "    FP = to_np((pred.mean(dim=0).argmax(dim=1) != y) & (y == 0))\n",
    "    FN = to_np((pred.mean(dim=0).argmax(dim=1) != y) & (y == 1))\n",
    "    \n",
    "    result = metric(pred)\n",
    "    \n",
    "    TP_result = result[TP]\n",
    "    TN_result = result[TN]\n",
    "    FP_result = result[FP]\n",
    "    FN_result = result[FN]\n",
    "    \n",
    "    fig,ax = plt.subplots(2,2,figsize=figsize)\n",
    "    \n",
    "    sns.distplot(TP_result,ax=ax[0,0],bins=bins)\n",
    "    ax[0,0].set_title(f\"True positive\")\n",
    "    \n",
    "    sns.distplot(TN_result,ax=ax[0,1],bins=bins)\n",
    "    ax[0,1].set_title(f\"True negative\")\n",
    "    \n",
    "    sns.distplot(FP_result,ax=ax[1,0],bins=bins)\n",
    "    ax[1,0].set_title(f\"False positive\")\n",
    "    \n",
    "    sns.distplot(FN_result,ax=ax[1,1],bins=bins)\n",
    "    ax[1,1].set_title(f\"False negative\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get predictions for a test set\n",
    "This patches a method to learner to make mc dropout predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def bayes_get_preds(self:Learner, ds_idx=1, dl=None, n_sample=10, \n",
    "                    act=None,with_loss=False, **kwargs):\n",
    "    \"\"\"Get MC Dropout predictions from a learner, and eventually reduce the samples\"\"\"  \n",
    "    cbs = [MCDropoutCallback()]\n",
    "    if 'cbs' in kwargs:\n",
    "        kw_cbs = kwargs.pop('cbs') \n",
    "        if 'MCDropoutCallback' not in L(kw_cbs).attrgot('name'):\n",
    "            cbs = kw_cbs + cbs\n",
    "    preds = []        \n",
    "    with self.no_bar():\n",
    "        for i in range(n_sample):\n",
    "            pred, y = self.get_preds(ds_idx=ds_idx,dl=dl,act=act,\n",
    "                                     with_loss=with_loss, cbs=cbs, **kwargs)\n",
    "            # pred = n_dl x n_vocab\n",
    "            preds.append(pred)\n",
    "    preds = torch.stack(preds)\n",
    "    ents = entropy(preds)\n",
    "    mean_preds = preds.mean(dim=0)\n",
    "    max_preds = mean_preds.max(dim=1)\n",
    "    best_guess = max_preds.indices\n",
    "    best_prob = max_preds.values\n",
    "    best_cat = L(best_guess,use_list=True).map(lambda o: self.dls.vocab[o.item()])\n",
    "    return preds, mean_preds, ents,best_guess, best_prob, best_cat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get predictions for an image item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def bayes_predict(self:Learner,item, rm_type_tfms=None, with_input=False,\n",
    "                  sample_size=10,reduce=True):\n",
    "    \"gets a sample distribution of predictions and computes entropy\"\n",
    "    dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    \n",
    "    # modify get_preds to get distributed samples\n",
    "    collect_preds = []\n",
    "    collect_targs = []\n",
    "    collect_dec_preds = []\n",
    "    collect_inp = None\n",
    "    cbs = [MCDropoutCallback()]\n",
    "    with self.no_bar():\n",
    "        for j in range(sample_size):\n",
    "            inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True,\n",
    "                                                   with_decoded=True, \n",
    "                                                   cbs=cbs)\n",
    "            i = getattr(self.dls, 'n_inp', -1)\n",
    "            inp = (inp,) if i==1 else tuplify(inp)\n",
    "            dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
    "            dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "            # res = dec_targ,dec_preds[0],preds[0]\n",
    "            if with_input and collect_inp is None: # collect inp first iter only\n",
    "                   collect_inp = dec_inp                                     \n",
    "            collect_targs.append(dec_targ)\n",
    "            collect_dec_preds.append(dec_preds[0])\n",
    "            collect_preds.append(preds[0])\n",
    "    dist_preds = torch.stack(collect_preds) \n",
    "    dist_dec_preds = L(collect_dec_preds).map(lambda o: o.item())\n",
    "    dist_targs = L(collect_targs)\n",
    "    res1 = (dist_targs, dist_dec_preds, dist_preds) \n",
    "    \n",
    "    mean_pred = dist_preds.mean(dim=0)\n",
    "    ent = entropy(dist_preds.unsqueeze(1)).item()\n",
    "    best_guess = torch.argmax(mean_pred).item()\n",
    "    best_prob = mean_pred[best_guess].item()\n",
    "    best_cat = self.dls.vocab[best_guess]\n",
    "    res2 = (ent, best_prob, best_guess, best_cat)\n",
    "    \n",
    "    if reduce:\n",
    "        if len(dist_targs.unique()) > 1:\n",
    "            targ = Counter(dist_targs)\n",
    "        else:\n",
    "            targ = dist_targs.unique()[0]\n",
    "            \n",
    "        if len(dist_dec_preds.unique()) > 1:\n",
    "            dec_pred = Counter(dist_dec_preds)\n",
    "        else:\n",
    "            dec_pred = dist_dec_preds.unique()[0]\n",
    "        res1 = (targ, dec_pred, mean_pred)\n",
    "    \n",
    "    res = res1 + res2\n",
    "    if with_input:\n",
    "        res = (collect_inp,) + res\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add uncertainty threshold to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def bayes_predict_with_uncertainty(self:Learner, item, rm_type_tfms=None, with_input=False, threshold_entropy=0.2, sample_size=10, reduce=True):\n",
    "    \"gets prediction results plus if prediction passes entropy threshold\"\n",
    "    res = self.bayes_predict(item,rm_type_tfms=rm_type_tfms, \n",
    "                             with_input=with_input, sample_size=sample_size, \n",
    "                             reduce=reduce)\n",
    "    ent = res[4] if with_input else res[3]\n",
    "    return (ent < threshold_entropy,) + res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.test_utils import synth_dbunch, synth_learner\n",
    "try:\n",
    "    from contextlib import nullcontext # python 3.7 only\n",
    "except ImportError as e:\n",
    "    from contextlib import suppress as nullcontext # supported in 3.6 below\n",
    "dls = synth_dbunch()\n",
    "dls.vocab = [1,]\n",
    "learner = synth_learner(data=dls)\n",
    "learner.no_bar = nullcontext\n",
    "bears_dl = dls.train\n",
    "pets_dl = dls.valid\n",
    "N_SAMPLE = 2\n",
    "CATEGORIES = 1\n",
    "BS = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local\n",
    "from fastai.learner import load_learner\n",
    "from fastai.data.transforms import get_image_files\n",
    "from fastai.data.external import Config\n",
    "from fastai.vision.core import PILImage\n",
    "import random\n",
    "# setup objects using local paths\n",
    "cfg = Config()\n",
    "learner = load_learner(cfg.model_path/'bears_classifier'/'export.pkl')\n",
    "bear_path = cfg.data_path/'bears'\n",
    "pet_path = cfg.data_path/'pets'\n",
    "bear_img_files = get_image_files(bear_path)\n",
    "pet_img_files = get_image_files(pet_path)\n",
    "\n",
    "random.seed(69420) # fix images retrieved\n",
    "pet_img = PILImage.create(pet_img_files.shuffle()[0])\n",
    "bear_img = PILImage.create(bear_img_files.shuffle()[0])\n",
    "\n",
    "pet_items = pet_img_files.shuffle()[:20]\n",
    "bear_items = bear_img_files.shuffle()[:20]\n",
    "\n",
    "pet_dset = pet_items.map(lambda o: PILImage.create(o))\n",
    "bear_dset = bear_items.map(lambda o: PILImage.create(o))\n",
    "pets_dl = learner.dls.test_dl(pet_dset,num_workers=0)\n",
    "\n",
    "bears_dl = learner.dls.test_dl(bear_dset,num_workers=0)\n",
    "# xb.shape = torch.size([20,3,224,224])\n",
    "N_SAMPLE = 2\n",
    "CATEGORIES = 3\n",
    "BS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayes Prediction for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_res = learner.bayes_get_preds(dl=bears_dl, n_sample=N_SAMPLE)\n",
    "pet_res = learner.bayes_get_preds(dl=pets_dl, n_sample=N_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds, mean_preds, ents,best_guess, best_prob, best_cat \n",
    "test_eq(len(bear_res),6)\n",
    "# ci 6\n",
    "# local 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "test_eq(bear_res[0].shape, [N_SAMPLE,BS,CATEGORIES])\n",
    "#ci torch.Size([2, 160, 1])\n",
    "#local torch.Size([5, 20, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean predictions\n",
    "test_eq(bear_res[1].shape, [BS, CATEGORIES])\n",
    "#ci torch.Size([160, 1])\n",
    "#local torch.Size([20, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "test_eq(bear_res[2].shape,[BS])\n",
    "#ci torch.Size([160])\n",
    "#local torch.Size([20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best guess (index of mean)\n",
    "test_eq(bear_res[3].shape,[BS])\n",
    "# ci torch.Size([160])\n",
    "# local torch.Size([20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best probability (mean prediction)\n",
    "test_eq(bear_res[4].shape,[BS]) \n",
    "#ci torch.Size([160])\n",
    "#local torch.Size([20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best category (mean prediction)\n",
    "test_eq(len(bear_res[5]),BS)\n",
    "# ci 160\n",
    "# local 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
